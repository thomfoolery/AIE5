{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckbbj5diaHkg"
      },
      "source": [
        "# Fine-tuning Embeddings for RAG on Specific Data\n",
        "\n",
        "As we start our \"fine-tuning\" week, we'll start with the lowest hanging improvement one can do for RAG - which is:\n",
        "\n",
        "Fine-tuning embeddings!\n",
        "\n",
        "- 🤝 Breakout Room #1:\n",
        "  - Task 1: Dependencies and Boilerplate\n",
        "  - Task 2: Loading Data\n",
        "  - Task 3: Constructing a Fine-tuning Dataset\n",
        "  - Task 4: Fine-tuning `snowflake-arctic-embed-l`\n",
        "  - Task 5: Evaluating our Retriever\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xwor_3X6ODX"
      },
      "source": [
        "#### Basic Overview of Fine-tuning Embeddings\n",
        "\n",
        "In essence, what we want to do when we fine-tune our embedding models is very simple:\n",
        "\n",
        "```\n",
        "Move the embeddings for questions relating to a document\n",
        "closer together with that document\n",
        "```\n",
        "\n",
        "We can think of fine-tuning our embedding models as follows:\n",
        "\n",
        "1) We have some pair of text items that *should* be closer together\n",
        "  - `Question`, `Document` pairs\n",
        "  - EX: `Who drives the bus?`, `The bus was driven by Kyle, the Bus Driver`.\n",
        "\n",
        "2) We use these pairs as labeled data to fine-tune our embedding model.\n",
        "\n",
        "The process of training helps the model more accurately associate our questions with the correct documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX5R3HVz6FOQ"
      },
      "source": [
        "##### ❓ Question #1:\n",
        "\n",
        "Describe the nuance between using Q&D pairs to train the embedding model vs. inter-document pairs/related sentences.\n",
        "\n",
        "What caveats does this approach have? Are there any special considerations for what kind of Q's we should use?\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER:**\n",
        "\n",
        "We are specifically relating *the questions* to *the documents*. This means that we are making our embedding model at the very specific task of relating potential questions to specific documents.\n",
        "\n",
        "There are many caveats, but the main ones are:\n",
        "\n",
        "- Your Q's should reflect the Q's of your users\n",
        "- This kind of fine-tuning will (purposefully) \"overfit\" on your data; this is the desired result in this case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NkSaurzbpyS"
      },
      "source": [
        "## Task 1: Dependencies and Boilerplate\n",
        "\n",
        "We'll set up our `nest_asyncio` so we can leverage async loops in our Notebook.\n",
        "\n",
        "We'll also install the required libraries we'll be using today, and set up our OpenAI API key!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c_EUibmcDU3"
      },
      "source": [
        "### Nest Asyncio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "zq-6s7LbPnKH"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8uFz8RVcFFu"
      },
      "source": [
        "### Install Dependencies\n",
        "\n",
        ">> NOTE: You do not need to do these steps if you are running this notebook locally with `uv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulZIBA1ZoSsV",
        "outputId": "12d9c766-843f-40bf-bdf8-e0ed04b6d87f"
      },
      "outputs": [],
      "source": [
        "#!pip install -qU langchain_openai langchain_huggingface langchain_core langchain langchain_community langchain-text-splitters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3GFD7B-tOCrx"
      },
      "outputs": [],
      "source": [
        "#!pip install -qU faiss-cpu python-pptx==1.0.2 nltk==3.9.1 pymupdf beautifulsoup4 lxml "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FM-eUlrcI8a"
      },
      "source": [
        "### Provide OpenAI API Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wA_mlurVqtrp",
        "outputId": "18cccb1e-095f-40fa-def5-2454f9bcdcae"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter Your OpenAI API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFZ217gCDVTr"
      },
      "source": [
        "## Task 2: Loading Data\n",
        "\n",
        "We'll prepare our data - and download our webpages which we'll be using for our data today.\n",
        "\n",
        "These webpages are from [Simon Willison's](https://simonwillison.net/) yearly \"AI learnings\".\n",
        "\n",
        "- [2023 Blog](https://simonwillison.net/2023/Dec/31/ai-in-2023/)\n",
        "- [2024 Blog](https://simonwillison.net/2024/Dec/31/llms-in-2024/)\n",
        "\n",
        "Let's start by collecting our data into a useful pile!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: data: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 31427    0 31427    0     0   221k      0 --:--:-- --:--:-- --:--:--  222k\n"
          ]
        }
      ],
      "source": [
        "!curl https://simonwillison.net/2023/Dec/31/ai-in-2023/ -o data/2023_llms.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 70286    0 70286    0     0   559k      0 --:--:-- --:--:-- --:--:--  562k\n"
          ]
        }
      ],
      "source": [
        "!curl https://simonwillison.net/2024/Dec/31/llms-in-2024/ -o data/2024_llms.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DHJhTzsvN75t"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "from langchain_community.document_loaders import BSHTMLLoader\n",
        "\n",
        "path = \"data/\"\n",
        "text_loader = DirectoryLoader(path, glob=\"*.html\", loader_cls=BSHTMLLoader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UbKa6-V0nvp"
      },
      "source": [
        "Next, we'll set up a classic naive chunking strategy as we only care that the documents get parsed into chunks that we can generate synthetic questions about."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NsPrOOqXOsNX"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 750,\n",
        "    chunk_overlap  = 20,\n",
        "    length_function = len\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lf_PoX7l09Rg"
      },
      "source": [
        "Next we can load/split these documents as follows.\n",
        "\n",
        ">> NOTE: You may need to run this cell twice to get it to work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OMYPX6N6Os8M"
      },
      "outputs": [],
      "source": [
        "training_documents = text_splitter.split_documents(text_loader.load())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAozuMoNOvnp",
        "outputId": "dc1d663e-7153-4c51-cedb-d1bc3888c4ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "102"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(training_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yE2TFIq1BuJ"
      },
      "source": [
        "Next, we're going to associate each of our chunks with a unique identifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "AwyIForybIpo"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "\n",
        "id_set = set()\n",
        "\n",
        "for document in training_documents:\n",
        "  id = str(uuid.uuid4())\n",
        "  while id in id_set:\n",
        "    id = uuid.uuid4()\n",
        "  id_set.add(id)\n",
        "  document.metadata[\"id\"] = id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJnL4oNg341U"
      },
      "source": [
        "Next, we'll simply use naive Python slicing to create a training, test, and validation set to prepare our data for the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MTS4GTSEcnG4"
      },
      "outputs": [],
      "source": [
        "training_split_documents = training_documents[:len(training_documents) - 24]\n",
        "val_split_documents = training_documents[len(training_documents) - 24:102-12]\n",
        "test_split_documents = training_documents[102-12:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzlvKbONDWvQ"
      },
      "source": [
        "## Task 3: Constructing a Fine-tuning Dataset\n",
        "\n",
        "Using the nodes we created above, we can finally start constructing a fine-tuning dataset utilizing OpenAI's `gpt-4o-mini` (released [today](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/)).\n",
        "\n",
        "The basic idea here is straightforward enough:\n",
        "\n",
        "1. We look at a document\n",
        "2. We generate questions that could be answered by that node\n",
        "\n",
        "This gives us a number of question/context pairs that we can use to fine-tune our Embeddings model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_EWfmIscMrvg"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "qa_chat_model = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-hLnsSB6Y-S"
      },
      "source": [
        "We'll create a simple Question Generation prompt to query `gpt-4o-mini` to generate Questions for each retrieved context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "diEWcw00NMSj"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "qa_prompt = \"\"\"\\\n",
        "Given the following context, you must generate questions based on only the provided context.\n",
        "\n",
        "You are to generate {n_questions} questions which should be provided in the following format:\n",
        "\n",
        "1. QUESTION #1\n",
        "2. QUESTION #2\n",
        "...\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "qa_prompt_template = ChatPromptTemplate.from_template(qa_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u87Izpgm6_fk"
      },
      "source": [
        "We'll create a simple chain to query the LLM!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ggl9SSjiNbpG"
      },
      "outputs": [],
      "source": [
        "question_generation_chain = qa_prompt_template | qa_chat_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4duvHirh7DQv"
      },
      "source": [
        "There's a lot going on in this function - let's take a deeper look:\n",
        "\n",
        "1. First, we provide a list of documents and a number of questions\n",
        "2. We, for each document in our list, generate `n_questions` of questions.\n",
        "3. We then associate those questions and contexts via a `UUID`.\n",
        "\n",
        "> NOTE: The reason we're doing this `UUID` association is for ease of use later in the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Lm2JvgC9X37"
      },
      "source": [
        "##### 🏗️ Activity #1:\n",
        "\n",
        "We have:\n",
        "\n",
        "- Lists of `Documents` with the `metadata` field `id`.\n",
        "\n",
        "We need:\n",
        "\n",
        "- An object with key `id`, which have values `str` questions.\n",
        "- An object with key `question_id`, which have values `List(str)` which will be a list of associated `context_id`.\n",
        "\n",
        "An Example:\n",
        "\n",
        "question_object:\n",
        "```python\n",
        "{\n",
        "'b4b95fb6-f827-4454-aa5b-20e62733f172': 'What types of accessible formats are available for persons with disabilities?',\n",
        "'df58ee4f-714c-419e-8324-94e5870574e2': 'How do accessible formats benefit persons with disabilities?',\n",
        "'505fce8b-0e56-48de-a251-61027e396918': 'What are some of the risks associated with the increasing capabilities of AI systems that generate synthetic content?',\n",
        "'8ff0ab33-60dc-4fee-8958-91bfb686aca8': 'Why is it important for providers of AI systems to embed technical solutions for marking and detecting synthetic content?'\n",
        "}\n",
        " ```\n",
        "\n",
        " context_object:\n",
        " ```python\n",
        "{\n",
        "'b4b95fb6-f827-4454-aa5b-20e62733f172': ['dd75bf94-75f3-4603-8e4b-5522f6925638'],\n",
        "'df58ee4f-714c-419e-8324-94e5870574e2': ['dd75bf94-75f3-4603-8e4b-5522f6925638'],\n",
        "'505fce8b-0e56-48de-a251-61027e396918': ['ffe3893f-688c-48e8-90bd-7a9feb953d90'],\n",
        "'8ff0ab33-60dc-4fee-8958-91bfb686aca8': ['ffe3893f-688c-48e8-90bd-7a9feb953d90'],\n",
        "}\n",
        " ```\n",
        "\n",
        " As you can see, a piece of context can be associated with more than 1 question.\n",
        "\n",
        " The task is to write the Python function(s) to accomplish this task.\n",
        "\n",
        " Your function signature is provided below, along with the desired return values.\n",
        "\n",
        " > NOTE: You can make any modifications that you desire - assuming that you have the correct input and outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "U4yi4NfTCnLc"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "import uuid\n",
        "\n",
        "async def create_questions(documents, n_questions):\n",
        "    questions = {}\n",
        "    relevant_docs = {}\n",
        "\n",
        "    for document in tqdm.tqdm(documents):\n",
        "        context = document.page_content\n",
        "        \n",
        "        # get questions by invoking the question generation chain\n",
        "        response = await question_generation_chain.ainvoke(\n",
        "            {\"context\": context, \"n_questions\": n_questions}\n",
        "        )\n",
        "        \n",
        "        # split the response into two questions\n",
        "        [question1, question2] = response.content.split(\"\\n\")\n",
        "        \n",
        "        # generate a unique id for the first question\n",
        "        id1 = str(uuid.uuid4())\n",
        "        while id1 in questions:\n",
        "            id1 = str(uuid.uuid4())\n",
        "        # store the first question\n",
        "        questions[id1] = question1[2:].strip()\n",
        "        \n",
        "        # generate a unique id for the second question\n",
        "        id2 = str(uuid.uuid4())\n",
        "        while id2 in questions:\n",
        "            id2 = str(uuid.uuid4())\n",
        "        # store the second question\n",
        "        questions[id2] = question2[2:].strip()\n",
        "        \n",
        "        # Store the relevant doc for each questions\n",
        "        relevant_docs[id1] = [document.metadata[\"id\"]]\n",
        "        relevant_docs[id2] = [document.metadata[\"id\"]]\n",
        "\n",
        "    return questions, relevant_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5W0eWOUo4QGL"
      },
      "source": [
        "### REMOVE `await` IF NOT USING ASYNC (HINT: Use `async`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85Dq6KRqEs0F",
        "outputId": "60dcd580-b6e8-4e3b-d605-05b492ca5c96"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 78/78 [01:35<00:00,  1.23s/it]\n"
          ]
        }
      ],
      "source": [
        "training_questions, training_relevant_contexts = await create_questions(training_split_documents, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'94787414-adea-43b9-99b4-11f3509e282f': 'What significant advancements in AI were made in 2023, particularly regarding Large Language Models (LLMs)?', '686d9eb9-7bf9-458a-a96b-af741ff97184': 'How does the development of LLMs in 2023 relate to the historical context of Artificial Intelligence since the 1950s?', 'acccd852-a3c2-4362-b1d6-0b6750de08d8': 'What are some potential applications of Large Language Models (LLMs) mentioned in the context?', 'c6135efa-282c-49aa-9031-3e07bc62829d': 'What is identified as the biggest unsolved problem related to LLMs?', '7b03f2d9-ebe4-4b66-b660-3558b5a343aa': 'What are some of the capabilities of Large Language Models (LLMs) mentioned in the context?', '5901474c-4c09-47d4-8385-ef9460a02c59': 'What potential negative uses of LLMs are highlighted in the provided context?', 'b244bcf4-23da-4266-85e6-cea964659c9a': 'What are some ways the author has used LLMs to improve their productivity and entertainment?', 'bbec0f1f-068f-488c-b5c9-901303e7e60e': 'What concerns do some people have regarding the value and impact of LLMs?', '5bd48783-3c07-435e-8709-5802f0e43570': 'What is the primary factor that determines the quality of a resulting model when training a system?', 'f848f574-54f2-4be9-9be2-155027cb0edf': 'How many lines of code are typically needed to train a basic version of a powerful system, according to the context?', '24f24a74-6455-4664-93b8-87a7f3e8ef35': 'Which organizations have produced models that are better than GPT-3 in the past year?', '1a99a767-b59b-4030-87d7-4781ef5ac418': 'What is the estimated training cost for models like Microsoft’s Phi-2 using current Lambda pricing?', '5e8e4ed5-6b71-4f18-888c-9d2f8b46acab': 'What comparison does the author make to illustrate the difficulty of training an LLM?', '69e8a6d1-1c1d-472d-8619-ee3b558eb3c6': \"As of January of this year, what was the author's perception of the feasibility of running a useful LLM on personal computers?\", '87c93e24-bd3b-46bd-8bb6-f49d88f65181': 'What significant event occurred in February related to Meta and Llama?', '3b2aac2e-2823-4adb-9905-f41a99f2c527': 'How did the release of Llama 2 in July impact the use of large language models?', 'b95e8bca-703e-4805-898c-8a704a5726fd': 'What is the name of the model that the author runs on their iPhone?', '4476b1da-0739-4b26-b2b2-64d622b5d238': 'How can hobbyists create their own fine-tuned models according to the context?', 'ce8732ab-23b0-401d-a3b4-ae83495a6fd5': 'What is the role of the Hugging Face Open LLM Leaderboard in the ecosystem of model training and sharing?', '4fa480c9-229f-4ce5-aa6d-8f6846a5b054': 'How do fine-tuned community models compare to foundation models in terms of performance and licensing?', 'ecf35f47-02c1-4012-9e01-b9c63923778f': 'What significant advancements have been made in AI models since the release of GPT-4 in March?', 'da90c6e8-65ad-4ae2-8b95-c84f5c10163e': 'What are the claims made by Google’s Gemini Ultra regarding its capabilities compared to GPT-4?', '2dafa94a-a96e-4feb-8e3b-a29b563de733': 'What challenges do computer scientists and software engineers face when working with large language models (LLMs)?', 'e7865299-fcda-43ea-bc6b-29acaeae7932': 'Why is it difficult to evaluate the performance of LLMs according to the context provided?', '8acff781-de07-405b-86a2-8c6ee27edb90': 'What challenges does the author face when evaluating the strengths and weaknesses of an LLM?', '3fe2481c-d195-4319-8b33-913647626383': 'How does the author feel about the current state of prompting and its effectiveness in working with LLMs?', '6d79889f-cda1-4299-9e1e-db7e01017e3a': 'What unexpected capabilities have been discovered in LLMs that surprise both users and trainers?', '402d42cb-302e-499c-9e00-d000cdfd52aa': \"Is there a possibility that ChatGPT's performance changes in December due to its hidden system prompt and user behavior around the holidays?\", 'd74b5970-19ea-49e1-87f3-5fea3ed7fccc': 'What is the term coined by the author to describe the issue of manipulating responses from AI systems?', 'd1f2c81a-e072-4020-bdcb-da9f0c878173': 'How does the author suggest that users can improve the quality of answers from AI?', '07f4a7bc-21ac-4be3-97cb-b60457856c75': 'Why is it important for language models to be gullible in certain contexts?', '0fc0847a-3c48-4e55-aca4-cd21e615e141': 'What are the potential drawbacks of having a language model that believes everything it is told?', '22103fa8-5224-4fe9-ad92-883117e37331': 'What are AI agents commonly understood to be, according to the context provided?', '45ecedba-f931-4549-8a8c-840efd624da8': 'Why does the author believe that gullibility may hinder the development of AI agents?', '9ed11619-a8e4-414e-b640-6d3266b48479': 'What are some examples of programming languages mentioned in the context that have simpler grammar rules compared to natural languages?', 'acafa9a1-ace0-4594-a8ca-67998db866a0': 'What is one of the significant weaknesses of LLMs as described in the context?', '6955a0c0-5e0c-4dc2-a1c9-95e888564f5a': 'How does the ability of LLMs to execute and debug code impact the role of software engineers?', '07e42093-1e45-453c-967c-0c1419861eb0': 'What are the implications of having a Code Interpreter equivalent for fact-checking natural language?', 'a8dc3e2f-1c61-49ec-8419-51e7d8da541e': 'How can software engineers leverage their knowledge to assist coding interns in solving problems more effectively?', '726ca4ce-45a3-41a0-8d1c-fc3220a5a46e': 'What ethical concerns arise from the use of unlicensed training data in the development of models like Stable Diffusion?', '27170098-c72f-4531-80c2-248ff17d2287': 'What is the main subject of the lawsuit launched by the New York Times against OpenAI and Microsoft?', '7c6d336e-2376-43ac-9b62-86bf5ca187a6': 'Why is the first few pages of the 69-page PDF considered particularly valuable for understanding the issues at hand?', 'be52d309-3c8e-4c5c-8d1f-1096c9be511d': \"What are the ethical implications of training AI models on individuals' content without their consent?\", '9b59af11-050f-4eba-97cf-d73c2f66686c': 'How has the increasing quality of AI models affected employment in creative fields such as copywriting, art, and translation?', '1a6380ba-5720-4bc6-addd-19eff959f408': 'What was the title of the article that received the highest number of visitors according to the provided analytics?', 'ecc71eb0-78eb-490b-a869-f099728bd1be': 'How many pageviews did the article titled \"Leaked Google document: \\'We Have No Moat, And Neither Does OpenAI\\'\" receive?', '36deb0fc-d3ca-40e8-aa56-62611a7d1e47': 'What are embeddings and why are they considered important in the context of LLMs?', '7c96bd3d-7690-4e43-8811-96575e68c412': 'How does the new llamafile improve the process of running an LLM on a personal computer?', '823324b1-341f-4428-8452-1023b26cb2d7': 'What is the significance of prompt engineering in DALL-E 3?', '2ada5006-3d84-4268-99ba-643901e36549': 'How does the vicuna-7b Large Language Model operate entirely in a browser?', '3cd86f7a-dbb1-46e9-aae2-ab82ce9b9350': 'What topics were covered in the annotated presentations given in 2023?', '6de09f6c-5eb6-4214-b83a-983972e4ceed': 'Which podcasts featured discussions about Large Language Models?', '1b2249fe-1151-4a21-b824-555d6537f5ba': 'What is the main topic discussed in the article posted on December 31, 2023?', '11a8145c-7cb5-4244-941d-ed78f4aa1e03': 'When is the next article in the series \"Things we learned about LLMs\" scheduled to be published?', '6c0b1650-1543-4e77-9690-bdaffa74b97f': 'What is the significance of the numbers associated with blogging, AI, generative AI, and LLMs in the context provided?', 'e3c37440-dfbe-4e65-af41-30e2d31ab84e': 'Who is Tom Scott, and what is the topic of discussion related to him in the context?', '32b6370e-7389-4a4a-bdd9-6393d3ad6daf': 'What key themes and pivotal moments in the field of Large Language Models were identified in 2024?', '1d85bf5b-906f-44ea-ba5f-1f8006a5876e': 'How does the review of 2024 compare to the review of 2023 regarding advancements in LLMs?', '3c63f342-44ef-439a-b129-032ca2337ab6': 'What advancements have been made in multimodal vision and audio/video capabilities in LLMs?', '18e723c8-2632-4d0b-a300-a2c3a0696ffb': 'How has the competition affected the pricing of LLMs and what impact did it have on universal access to these models?', '1de55c83-c6ef-4f51-ae2e-2907094376fa': 'What challenges are associated with using LLMs in 2024?', 'c7f3e9f7-f57d-4d7b-a97a-7138cfa19d2c': 'How is knowledge distribution described in the context of LLMs?', 'c908ad18-5278-4dde-affa-223b73dfb00d': 'What significant change occurred in the AI landscape regarding models surpassing GPT-4 in the past twelve months?', '5cb44781-f136-4f3a-88d7-5faed9788169': 'How many organizations currently have models that rank higher than the original GPT-4 on the Chatbot Arena Leaderboard?', '0b70c031-9092-4463-b213-04ff278d513d': 'What notable capabilities were introduced with Google’s Gemini 1.5 Pro in February?', '01a59e17-9086-4de1-9695-af92d384cc3e': 'How did the author contribute to the Google I/O opening keynote in May?', '5b8180aa-ee4e-40fa-ab51-669010ed3b85': 'What was the maximum token limit for most models last year before the introduction of Gemini 1.5 Pro?', '606bf43c-830c-487a-abb4-36f02c7b2167': \"How many tokens can Google's Gemini series accept?\", '2cd6bb1c-1021-4449-8f89-96fc3c31b5cf': 'How do longer inputs enhance the problem-solving capabilities of an LLM?', '59575383-b449-42de-972e-6e00d129e420': 'What are some examples of use-cases that benefit from providing long inputs to an LLM?', '1d155dca-e26f-4e13-82b3-b4f82771d7d1': \"What is the name of the model that quickly became the author's favorite daily-driver after its launch in March?\", '80d7fe01-11b9-4a97-87f1-f394cba43d36': 'What significant upgrade did Claude 3.5 Sonnet receive on October 22?', '251af14a-32d9-4b72-977e-aecfe42ef946': 'Which organization currently has a higher scoring model than GPT-4-0314 on the Chatbot Arena leaderboard?', '1c1c4450-0483-48f3-b01a-21ff13e8dfbe': 'How has the significance of training a GPT-4 beating model changed from 2023 to 2024?', '3b282527-e29a-431a-a654-9c4b73e009e6': 'What model of MacBook Pro is being used in the context, and what is its storage capacity?', 'c5a8b6be-dadd-4d02-869f-2b672f1ca7de': 'When did the author first run a large language model on their laptop?', 'f1bb24df-5d8e-42f1-b353-88ee3569187c': 'What is the licensing type of the Qwen2.5-Coder-32B model?', 'f287bf14-b9b6-4b76-9827-a03f346250f5': 'When was Meta’s Llama 3.3 70B model released?', 'df9ae603-0591-4431-b629-5bc1446097e7': 'What are the hardware requirements mentioned for running models like GPT-4?', 'be0edfc1-9cfd-4a7a-9cd6-f584596d6adb': 'What advancements have contributed to the improved performance of model training and inference over the past year?', '9d320374-577a-4b09-ba8d-e1edc0fd4315': \"What are the sizes of Meta's Llama 3.2 models mentioned in the context?\", 'be51070c-b666-4d19-b678-792b268999e8': 'How does the performance of the Llama 3.2 3B model compare to GPT-4 according to the context?', 'b14d46ba-c6c3-4d85-ba00-531967928d6c': \"What were the prices for OpenAI's GPT-4 and GPT-4 Turbo in December 2023?\", 'abb1a8a1-371d-4ce1-91ef-c8c34950e330': \"How much does OpenAI's GPT-4o cost per million tokens compared to GPT-4?\", 'ab59ddaf-b66a-48c7-9264-f1822981d7ab': 'What are the two main factors driving the price drops in LLM models according to the context?', 'c9e5bb4f-bed8-45f6-92ee-0a8ebf9ae4d7': 'How much does Google’s Gemini 1.5 Flash 8B cost per mTok compared to GPT-3.5 Turbo from last year?', '0d370168-6ac8-4b49-89ac-a51a955ba5b9': 'What is the total cost to generate short descriptions for all 68,000 photos using Google’s Gemini 1.5 Flash 8B model?', '5a37fe15-e1f3-4453-9502-3b8e343b465c': 'How many input tokens are required to generate descriptions for the entire personal photo library?', '62241c2e-af4a-4464-b89e-ec6fa05356e3': 'What is the total cost to process 68,000 images mentioned in the context?', '67f38981-0054-4ebc-96ff-abe26cd267cb': 'Which command was used to obtain descriptions of the image IMG_1825.jpeg?', '772e9d62-4aa6-4c9b-948b-d0c201b2e284': 'What colors are the markings on the dark brown/black butterfly in the feeder?', '62e120ea-5113-4057-ae68-cadf5c3deb91': 'What type of fruit is visible inside the shallow dish at the California Academy of Sciences?', '746ba953-2776-46d4-83a6-3d2fa0d2c629': 'What is the significance of the cost reduction mentioned in the context regarding LLMs in 2024?', '82e5901b-4f7b-4097-8cbe-7d97bfb9399f': 'How does the emergence of multi-modal LLMs, such as GPT-4 Vision and Google’s Gemini 1.0, reflect the trends in technology for 2024?', 'fe16867c-d127-4f39-af85-95ae1c913ac3': 'What significant multi-modal models were released by major vendors in 2024?', 'b7d5a4fa-8c78-4206-8a50-f7634123efca': 'Which upgrades were made to the LLM CLI tool in October 2024?', '192ffcdf-bde4-4047-bd8b-6f4e71c54bf3': 'What are some of the new capabilities of multi-modal models mentioned in the context?', '11a81fef-34c9-4d67-a4b6-a6d7303a0ca6': 'When did the ability to talk to ChatGPT first become available, and what technology was used to facilitate this feature?', '8aed659b-4b4d-46b0-84c0-20d1d92583cd': 'What is the significance of the \"o\" in the GPT-4o model?', '4c9d8e58-f042-41c6-95dc-e4e1c8117c9e': 'Why did the voice from the demo, named Skye, not make it to a production product?', 'be096a32-0dc8-40a2-9ae0-a5ac6598b7c0': 'What improvements were noted in the intonation of ChatGPT Advanced Voice mode during its rollout?', '69f7a753-0cb7-4470-82bb-ab854e141fb3': 'How did the user experiment with accents in the Advanced Voice mode?', '45a94bec-b815-4503-b5a7-d07ede65ee0e': 'What capabilities does Google’s Gemini have regarding audio input and output?', '8dace8ff-64bb-4894-a2aa-3766e396273b': \"When is Amazon's voice mode for Amazon Nova expected to roll out?\", 'b27c77e3-0f0d-416b-826f-33f9a6f78cd2': \"What new feature was introduced in ChatGPT's voice mode in December?\", 'c9f8c452-25fc-4c4d-a9b0-a669c307e6f7': \"How does Google Gemini's recent feature compare to ChatGPT's live video option?\", 'f0e2eab2-405b-4852-b0ca-67e75eb0ee3f': 'What are the new abilities mentioned in the context, and how long have they been available?', '72e94346-8a94-4ea4-95e7-e9388aa6ddef': 'How did OpenAI improve its API access for developers in December?', '96300f36-24b4-484a-9698-626a5432ddf0': 'What is the significance of Claude Artifacts in the context of LLMs and application development?', '4d8336ef-846e-4e08-aa9f-7745cca19d5f': 'How does Claude enable users to interact with applications created through its interface?', '7aec2500-53ea-4f41-a5f3-2c26b13d7002': 'What tools did the author describe in their writing about Claude Artifacts?', '61d423a4-ee9d-4574-8a55-6d6e62c3daa7': \"What features did GitHub and Mistral Chat introduce related to the author's findings?\", '7b378212-6a04-462e-bd4d-2832e195bead': 'What was introduced by the Chatbot Arena team in December regarding user interaction with models?', '8e5750fd-c055-49a4-947a-5f8c7198379f': 'How is the author planning to utilize prompts in their Datasette project?', '32938821-8b68-446e-87ba-5184a3e845dc': 'What are the names of the three best available models that were freely accessible for a few months this year?', 'e5535f55-09dc-42a3-ab06-64264d4afe99': 'In what year does the author expect the prompt-driven custom interface feature to be widely integrated into products?', 'a6ed50ed-32b4-4c6f-bc80-55a58547f631': \"What significant change occurred in May regarding OpenAI's GPT-4o model for users?\", 'a947cc6f-a505-4ceb-85f2-12d159eeb086': \"How does the launch of ChatGPT Pro affect access to OpenAI's most capable model compared to previous offerings?\", '647261b7-07bb-4c86-9922-086a8f0d8551': 'Why does the author find the term \"agents\" extremely frustrating?', '0c4885b1-ef19-47f2-adad-1fa457cbd221': 'What issue does the author highlight regarding the communication of information when someone claims to be building \"agents\"?', '77c8adfe-4eab-4cf4-b04c-03c99c2e7a05': 'What are the two main categories of thought regarding AI agents mentioned in the context?', '689a6484-1135-454f-a2a6-a1eb61048a11': 'How does the term \"autonomy\" relate to the discussion of AI agents in the provided context?', '66f90646-3dbd-4c9b-8fd9-c5fd0e3e9be0': 'What challenges do LLMs face in distinguishing truth from fiction?', '00a53753-fd51-4365-9584-416c43eadb59': 'What incident involving Google Search is mentioned in the context?', '1d1de089-faa9-438c-bf3a-2805e1dd040d': 'What is the relationship between prompt injection and gullibility as discussed in the context?', '56a8cd72-d897-440c-bc72-085a88c7fc93': 'How has the perception of \"agents\" evolved in relation to AGI since September 2022?', 'f13d6b43-8089-438d-8df2-4b8f8ef6f2fc': 'What is the key principle behind creating effective system prompts according to the context?', '153a3e1d-4bd0-4367-b9bd-56485d483e77': 'Why is having a strong evaluation suite important for building applications on LLM-powered systems?', '0cd00f5d-2856-4e69-988d-450a39face82': \"What concerns did @v0's initial release raise regarding prompt protection?\", '3dee43d6-07e3-4790-950e-36a657a2b2bb': 'How does the author compare a prompt without evals, models, and UX to an ASML machine?', '2871537b-5c0c-4ef3-8221-3662683e8c65': 'What challenges did the author face last year regarding their choice of platform for trying out new models?', '18af86eb-477b-4f98-a51d-99982a54a1d0': 'How does the author feel about their choice of platform as a Mac user this year compared to last year?', '25285c9b-1a9f-4161-a8f0-e2cff56c35a6': 'What advantages does a 64GB Mac have for running models compared to other machines?', '491f0e60-3f35-4a4e-b912-cbdf74a503fe': 'How does the mlx-lm Python library enhance the performance of MLX-compatible models on a Mac?', '7ab1f9a7-4e7d-417e-8af8-02edb56b2180': 'What is the mlx-vlm project and how does it relate to vision LLMs on Apple Silicon?', '4249c70e-e819-4a7e-abc3-c60793f2e75a': 'What were the initial expectations for Apple\\'s \"Apple Intelligence\" features, and how have they been perceived since their announcement?', '6ae6776e-01da-4e43-89bf-720d2766bea6': \"What are the limitations of Apple's LLM features compared to frontier LLMs, according to the context?\", 'c877a3b7-6e8d-441a-94eb-d2f5058aaf85': 'What notable development in LLM technology occurred in the final quarter of 2024?', 'aba199c3-5b6c-4f2c-810d-1548fbb1af34': 'What is the main concept behind the chain-of-thought prompting trick as discussed in the context?', 'dd2ee359-2938-47a5-820d-1cff10b7c42f': 'How do o1 models differ from traditional models in terms of reasoning and output visibility?', '462b3993-5453-4d4a-a71d-4bdcecf675b8': 'What is the main innovation mentioned in the context regarding model scaling?', '94ce2715-b85a-4b9d-a606-6b7d40badb84': 'When was the sequel to o1, o3 announced, and what benchmark did it achieve impressive results against?', '3da70132-ecb1-4270-8442-114da659a535': 'When did Google release their gemini-2.0-flash-thinking-exp model?', '13b40c4b-a1f7-41d4-b4bf-955cda1e4669': \"What is the license under which Alibaba's QwQ model was released?\", '7c903a60-6acd-4606-ae98-ac35ac2ce623': 'What significant event related to DeepSeek v3 occurred on Christmas Day?', '5bd00fb4-95de-4a18-a519-66fd99b5b799': 'What was the title of the paper published by Meta in December regarding large language models?', '956c8a54-b521-480f-b92b-04a0943de1cc': \"How does the parameter size of DeepSeek v3 compare to that of Meta's Llama 3.1?\", 'e2e06cba-a459-4e68-a432-37e1d596ce00': 'What is the estimated training cost of DeepSeek v3, and how does it compare to the training hours of Llama 3.1?', '66f683bc-40fd-4562-ae50-062f698ba10d': 'How have US export regulations on GPUs to China influenced training optimizations in AI models?', 'c201348b-be5f-41b6-bc16-846aadcb53e7': 'What changes have occurred in the energy usage and environmental impact of running AI prompts over the past couple of years?', 'e7ab7e01-6433-4dfa-8608-2e5813f4f9a6': 'How does the energy consumption of individual prompts compare to other activities like driving a car or watching a video on YouTube?', '9c937e71-75ca-4d23-a239-3ad0abc3522a': 'What does the cost of training the DeepSeek v3 model suggest about the future of training costs for AI models?', 'e1eac613-f5f7-4dc0-b3e2-f84c14738ef7': 'What are the potential environmental impacts of the competitive buildout of infrastructure by major tech companies?', 'a77d4d33-f5ad-4109-bfe6-1078e5ba3c6c': 'How might the training costs of models like DeepSeek v3 influence the necessity of building new datacenters?', 'ae2453c3-ec1d-4e55-87b4-75dd3572ef6b': 'What were some of the financial crashes associated with the construction of railways in the 1800s?', 'ae3d3a47-3bae-4013-a11f-30edf67c8614': 'How did the construction of railways in the 1800s impact the environment?'}\n",
            "{'94787414-adea-43b9-99b4-11f3509e282f': ['52ef1991-5950-4f17-a7a5-e2b6fd32fcaf'], '686d9eb9-7bf9-458a-a96b-af741ff97184': ['52ef1991-5950-4f17-a7a5-e2b6fd32fcaf'], 'acccd852-a3c2-4362-b1d6-0b6750de08d8': ['10f3d2f2-210b-4012-8067-9cb907bb5db0'], 'c6135efa-282c-49aa-9031-3e07bc62829d': ['10f3d2f2-210b-4012-8067-9cb907bb5db0'], '7b03f2d9-ebe4-4b66-b660-3558b5a343aa': ['ad3d33f7-9c17-4a83-bf3c-ca4cb99d28d3'], '5901474c-4c09-47d4-8385-ef9460a02c59': ['ad3d33f7-9c17-4a83-bf3c-ca4cb99d28d3'], 'b244bcf4-23da-4266-85e6-cea964659c9a': ['10be4530-b041-4958-b189-0065e893df50'], 'bbec0f1f-068f-488c-b5c9-901303e7e60e': ['10be4530-b041-4958-b189-0065e893df50'], '5bd48783-3c07-435e-8709-5802f0e43570': ['0e3d3566-05f1-45e9-84bc-0aeb5251caa1'], 'f848f574-54f2-4be9-9be2-155027cb0edf': ['0e3d3566-05f1-45e9-84bc-0aeb5251caa1'], '24f24a74-6455-4664-93b8-87a7f3e8ef35': ['fdd35854-ea01-4892-9707-a6a638d8a617'], '1a99a767-b59b-4030-87d7-4781ef5ac418': ['fdd35854-ea01-4892-9707-a6a638d8a617'], '5e8e4ed5-6b71-4f18-888c-9d2f8b46acab': ['46174b88-7158-4a1b-a862-466098b1da61'], '69e8a6d1-1c1d-472d-8619-ee3b558eb3c6': ['46174b88-7158-4a1b-a862-466098b1da61'], '87c93e24-bd3b-46bd-8bb6-f49d88f65181': ['2479167c-580a-47c2-9c8e-8902cf9cd7e9'], '3b2aac2e-2823-4adb-9905-f41a99f2c527': ['2479167c-580a-47c2-9c8e-8902cf9cd7e9'], 'b95e8bca-703e-4805-898c-8a704a5726fd': ['fce2290c-64bb-4214-8eee-476997f657f1'], '4476b1da-0739-4b26-b2b2-64d622b5d238': ['fce2290c-64bb-4214-8eee-476997f657f1'], 'ce8732ab-23b0-401d-a3b4-ae83495a6fd5': ['728de7e9-fccd-4148-be04-02b4cc8207e1'], '4fa480c9-229f-4ce5-aa6d-8f6846a5b054': ['728de7e9-fccd-4148-be04-02b4cc8207e1'], 'ecf35f47-02c1-4012-9e01-b9c63923778f': ['116fae12-479d-476b-9787-e18a34fafd69'], 'da90c6e8-65ad-4ae2-8b95-c84f5c10163e': ['116fae12-479d-476b-9787-e18a34fafd69'], '2dafa94a-a96e-4feb-8e3b-a29b563de733': ['1505eee7-1b9b-4ed2-97af-70a19600ed30'], 'e7865299-fcda-43ea-bc6b-29acaeae7932': ['1505eee7-1b9b-4ed2-97af-70a19600ed30'], '8acff781-de07-405b-86a2-8c6ee27edb90': ['203615ea-c909-47cf-ad2b-883d1b4da186'], '3fe2481c-d195-4319-8b33-913647626383': ['203615ea-c909-47cf-ad2b-883d1b4da186'], '6d79889f-cda1-4299-9e1e-db7e01017e3a': ['66de7085-cffe-4b91-abd4-08d092707a71'], '402d42cb-302e-499c-9e00-d000cdfd52aa': ['66de7085-cffe-4b91-abd4-08d092707a71'], 'd74b5970-19ea-49e1-87f3-5fea3ed7fccc': ['dfc1ac98-bc3c-4a7e-8119-5b5133e0a2ab'], 'd1f2c81a-e072-4020-bdcb-da9f0c878173': ['dfc1ac98-bc3c-4a7e-8119-5b5133e0a2ab'], '07f4a7bc-21ac-4be3-97cb-b60457856c75': ['d16926c7-93f8-40bb-a06c-dc385ceb20a2'], '0fc0847a-3c48-4e55-aca4-cd21e615e141': ['d16926c7-93f8-40bb-a06c-dc385ceb20a2'], '22103fa8-5224-4fe9-ad92-883117e37331': ['6130ab99-8be6-4511-9c39-105edcc2e729'], '45ecedba-f931-4549-8a8c-840efd624da8': ['6130ab99-8be6-4511-9c39-105edcc2e729'], '9ed11619-a8e4-414e-b640-6d3266b48479': ['9111d314-4ced-4968-9063-4d7cd4089fc1'], 'acafa9a1-ace0-4594-a8ca-67998db866a0': ['9111d314-4ced-4968-9063-4d7cd4089fc1'], '6955a0c0-5e0c-4dc2-a1c9-95e888564f5a': ['d17b2f20-ced0-4789-a007-52d01ac7e7d7'], '07e42093-1e45-453c-967c-0c1419861eb0': ['d17b2f20-ced0-4789-a007-52d01ac7e7d7'], 'a8dc3e2f-1c61-49ec-8419-51e7d8da541e': ['3bb629a3-fa6e-4a46-ba90-7c63436e8599'], '726ca4ce-45a3-41a0-8d1c-fc3220a5a46e': ['3bb629a3-fa6e-4a46-ba90-7c63436e8599'], '27170098-c72f-4531-80c2-248ff17d2287': ['7254cd99-dc5f-462a-90ae-cfaea1f4c4f1'], '7c6d336e-2376-43ac-9b62-86bf5ca187a6': ['7254cd99-dc5f-462a-90ae-cfaea1f4c4f1'], 'be52d309-3c8e-4c5c-8d1f-1096c9be511d': ['d3ab7fcd-58b1-42c4-b00f-03713869e253'], '9b59af11-050f-4eba-97cf-d73c2f66686c': ['d3ab7fcd-58b1-42c4-b00f-03713869e253'], '1a6380ba-5720-4bc6-addd-19eff959f408': ['f4389f3d-0efa-4e6b-b506-bc637ff60fb0'], 'ecc71eb0-78eb-490b-a869-f099728bd1be': ['f4389f3d-0efa-4e6b-b506-bc637ff60fb0'], '36deb0fc-d3ca-40e8-aa56-62611a7d1e47': ['cc92e997-77d2-4f9c-a04a-28ad1cb1dc7a'], '7c96bd3d-7690-4e43-8811-96575e68c412': ['cc92e997-77d2-4f9c-a04a-28ad1cb1dc7a'], '823324b1-341f-4428-8452-1023b26cb2d7': ['95efd7db-2f4c-471d-9eda-beb774ebd7f4'], '2ada5006-3d84-4268-99ba-643901e36549': ['95efd7db-2f4c-471d-9eda-beb774ebd7f4'], '3cd86f7a-dbb1-46e9-aae2-ab82ce9b9350': ['aeaf0c13-a850-4e1c-ae59-c506efc7d5e2'], '6de09f6c-5eb6-4214-b83a-983972e4ceed': ['aeaf0c13-a850-4e1c-ae59-c506efc7d5e2'], '1b2249fe-1151-4a21-b824-555d6537f5ba': ['41a7d0f2-137d-4457-a27c-1c74220324a1'], '11a8145c-7cb5-4244-941d-ed78f4aa1e03': ['41a7d0f2-137d-4457-a27c-1c74220324a1'], '6c0b1650-1543-4e77-9690-bdaffa74b97f': ['4443d6ca-6711-44de-afeb-836af29a4132'], 'e3c37440-dfbe-4e65-af41-30e2d31ab84e': ['4443d6ca-6711-44de-afeb-836af29a4132'], '32b6370e-7389-4a4a-bdd9-6393d3ad6daf': ['5989d940-2f8e-4ed1-9186-f931704278aa'], '1d85bf5b-906f-44ea-ba5f-1f8006a5876e': ['5989d940-2f8e-4ed1-9186-f931704278aa'], '3c63f342-44ef-439a-b129-032ca2337ab6': ['0ba55eaa-5fdb-4728-b712-c63777960183'], '18e723c8-2632-4d0b-a300-a2c3a0696ffb': ['0ba55eaa-5fdb-4728-b712-c63777960183'], '1de55c83-c6ef-4f51-ae2e-2907094376fa': ['e7342f7a-9b59-4f60-8dd3-1f2933802cb3'], 'c7f3e9f7-f57d-4d7b-a97a-7138cfa19d2c': ['e7342f7a-9b59-4f60-8dd3-1f2933802cb3'], 'c908ad18-5278-4dde-affa-223b73dfb00d': ['64693e3c-f541-4c35-a9eb-717c2733624e'], '5cb44781-f136-4f3a-88d7-5faed9788169': ['64693e3c-f541-4c35-a9eb-717c2733624e'], '0b70c031-9092-4463-b213-04ff278d513d': ['5584c321-1c77-41bb-b22c-8b6b3c82ee05'], '01a59e17-9086-4de1-9695-af92d384cc3e': ['5584c321-1c77-41bb-b22c-8b6b3c82ee05'], '5b8180aa-ee4e-40fa-ab51-669010ed3b85': ['eb450a25-8a35-45f9-a60e-f397bdae5feb'], '606bf43c-830c-487a-abb4-36f02c7b2167': ['eb450a25-8a35-45f9-a60e-f397bdae5feb'], '2cd6bb1c-1021-4449-8f89-96fc3c31b5cf': ['d3d28efb-5880-4e6f-8e43-e322412f85d1'], '59575383-b449-42de-972e-6e00d129e420': ['d3d28efb-5880-4e6f-8e43-e322412f85d1'], '1d155dca-e26f-4e13-82b3-b4f82771d7d1': ['0ad954f1-fc5f-4f5b-b595-6110ac9e0ebf'], '80d7fe01-11b9-4a97-87f1-f394cba43d36': ['0ad954f1-fc5f-4f5b-b595-6110ac9e0ebf'], '251af14a-32d9-4b72-977e-aecfe42ef946': ['153a525b-85be-4191-b3a4-180241478500'], '1c1c4450-0483-48f3-b01a-21ff13e8dfbe': ['153a525b-85be-4191-b3a4-180241478500'], '3b282527-e29a-431a-a654-9c4b73e009e6': ['874bb935-e79c-404c-aacf-24b5033d548e'], 'c5a8b6be-dadd-4d02-869f-2b672f1ca7de': ['874bb935-e79c-404c-aacf-24b5033d548e'], 'f1bb24df-5d8e-42f1-b353-88ee3569187c': ['af6ea5db-63c3-4dbf-946e-7c7f10b89148'], 'f287bf14-b9b6-4b76-9827-a03f346250f5': ['af6ea5db-63c3-4dbf-946e-7c7f10b89148'], 'df9ae603-0591-4431-b629-5bc1446097e7': ['024864b9-54ee-4912-a693-4cc7846c4b5c'], 'be0edfc1-9cfd-4a7a-9cd6-f584596d6adb': ['024864b9-54ee-4912-a693-4cc7846c4b5c'], '9d320374-577a-4b09-ba8d-e1edc0fd4315': ['044a46eb-d448-4756-a8e2-3e9f97fbc448'], 'be51070c-b666-4d19-b678-792b268999e8': ['044a46eb-d448-4756-a8e2-3e9f97fbc448'], 'b14d46ba-c6c3-4d85-ba00-531967928d6c': ['31b599da-5635-4cf4-b9f7-424790fc4d38'], 'abb1a8a1-371d-4ce1-91ef-c8c34950e330': ['31b599da-5635-4cf4-b9f7-424790fc4d38'], 'ab59ddaf-b66a-48c7-9264-f1822981d7ab': ['66bf4408-3cce-4f52-8f41-1a49e4e5e83d'], 'c9e5bb4f-bed8-45f6-92ee-0a8ebf9ae4d7': ['66bf4408-3cce-4f52-8f41-1a49e4e5e83d'], '0d370168-6ac8-4b49-89ac-a51a955ba5b9': ['2d62d3a0-aeaf-4ee9-805e-6d637bc7b359'], '5a37fe15-e1f3-4453-9502-3b8e343b465c': ['2d62d3a0-aeaf-4ee9-805e-6d637bc7b359'], '62241c2e-af4a-4464-b89e-ec6fa05356e3': ['13804a05-2a1b-4ecc-b606-a266141790ad'], '67f38981-0054-4ebc-96ff-abe26cd267cb': ['13804a05-2a1b-4ecc-b606-a266141790ad'], '772e9d62-4aa6-4c9b-948b-d0c201b2e284': ['fc211aba-b135-4ab9-a68b-bc7e560716c3'], '62e120ea-5113-4057-ae68-cadf5c3deb91': ['fc211aba-b135-4ab9-a68b-bc7e560716c3'], '746ba953-2776-46d4-83a6-3d2fa0d2c629': ['6e7b03fc-7c94-4d04-bcdb-5f72018f5ae1'], '82e5901b-4f7b-4097-8cbe-7d97bfb9399f': ['6e7b03fc-7c94-4d04-bcdb-5f72018f5ae1'], 'fe16867c-d127-4f39-af85-95ae1c913ac3': ['1090b625-b8ee-490d-a1a3-2346bea7f2f8'], 'b7d5a4fa-8c78-4206-8a50-f7634123efca': ['1090b625-b8ee-490d-a1a3-2346bea7f2f8'], '192ffcdf-bde4-4047-bd8b-6f4e71c54bf3': ['38b53ebc-16e2-42a6-adfa-1d0f8fb6bfb6'], '11a81fef-34c9-4d67-a4b6-a6d7303a0ca6': ['38b53ebc-16e2-42a6-adfa-1d0f8fb6bfb6'], '8aed659b-4b4d-46b0-84c0-20d1d92583cd': ['d542714f-b1d5-4cc0-9cfc-556689d1ed79'], '4c9d8e58-f042-41c6-95dc-e4e1c8117c9e': ['d542714f-b1d5-4cc0-9cfc-556689d1ed79'], 'be096a32-0dc8-40a2-9ae0-a5ac6598b7c0': ['8f14e105-e47b-4769-9417-aa343d035612'], '69f7a753-0cb7-4470-82bb-ab854e141fb3': ['8f14e105-e47b-4769-9417-aa343d035612'], '45a94bec-b815-4503-b5a7-d07ede65ee0e': ['342be5a6-3bc6-4562-8895-ad29cdd18f00'], '8dace8ff-64bb-4894-a2aa-3766e396273b': ['342be5a6-3bc6-4562-8895-ad29cdd18f00'], 'b27c77e3-0f0d-416b-826f-33f9a6f78cd2': ['42cfe283-6db2-4db9-b598-68773d6b3493'], 'c9f8c452-25fc-4c4d-a9b0-a669c307e6f7': ['42cfe283-6db2-4db9-b598-68773d6b3493'], 'f0e2eab2-405b-4852-b0ca-67e75eb0ee3f': ['8dc4fbe2-301d-4066-b549-0128d3a09c87'], '72e94346-8a94-4ea4-95e7-e9388aa6ddef': ['8dc4fbe2-301d-4066-b549-0128d3a09c87'], '96300f36-24b4-484a-9698-626a5432ddf0': ['8cfd4e1e-4565-443c-ac08-ed1188dd8864'], '4d8336ef-846e-4e08-aa9f-7745cca19d5f': ['8cfd4e1e-4565-443c-ac08-ed1188dd8864'], '7aec2500-53ea-4f41-a5f3-2c26b13d7002': ['9b2bd86d-5268-478a-a1ed-7ce5de996064'], '61d423a4-ee9d-4574-8a55-6d6e62c3daa7': ['9b2bd86d-5268-478a-a1ed-7ce5de996064'], '7b378212-6a04-462e-bd4d-2832e195bead': ['f1554c66-c4f8-4c66-a66c-2151c78108d7'], '8e5750fd-c055-49a4-947a-5f8c7198379f': ['f1554c66-c4f8-4c66-a66c-2151c78108d7'], '32938821-8b68-446e-87ba-5184a3e845dc': ['4f81d8ea-bf33-4059-a830-9f1750ec5fff'], 'e5535f55-09dc-42a3-ab06-64264d4afe99': ['4f81d8ea-bf33-4059-a830-9f1750ec5fff'], 'a6ed50ed-32b4-4c6f-bc80-55a58547f631': ['4b31a1f2-56ca-488d-b391-63cbad67840a'], 'a947cc6f-a505-4ceb-85f2-12d159eeb086': ['4b31a1f2-56ca-488d-b391-63cbad67840a'], '647261b7-07bb-4c86-9922-086a8f0d8551': ['0d78f154-5a0c-4531-8c71-b50a21301f95'], '0c4885b1-ef19-47f2-adad-1fa457cbd221': ['0d78f154-5a0c-4531-8c71-b50a21301f95'], '77c8adfe-4eab-4cf4-b04c-03c99c2e7a05': ['3076f23d-4cfd-4b4d-bd23-ec1088002003'], '689a6484-1135-454f-a2a6-a1eb61048a11': ['3076f23d-4cfd-4b4d-bd23-ec1088002003'], '66f90646-3dbd-4c9b-8fd9-c5fd0e3e9be0': ['a8bbd4be-076d-4c10-aa0b-6273ec144e14'], '00a53753-fd51-4365-9584-416c43eadb59': ['a8bbd4be-076d-4c10-aa0b-6273ec144e14'], '1d1de089-faa9-438c-bf3a-2805e1dd040d': ['6a506194-89f6-4f4a-b7f4-6f8136d664bd'], '56a8cd72-d897-440c-bc72-085a88c7fc93': ['6a506194-89f6-4f4a-b7f4-6f8136d664bd'], 'f13d6b43-8089-438d-8df2-4b8f8ef6f2fc': ['7e7a6fa4-2d13-493d-9293-bfb6b154d464'], '153a3e1d-4bd0-4367-b9bd-56485d483e77': ['7e7a6fa4-2d13-493d-9293-bfb6b154d464'], '0cd00f5d-2856-4e69-988d-450a39face82': ['174f87a3-f61d-4bba-9cea-02d82c1dd5e7'], '3dee43d6-07e3-4790-950e-36a657a2b2bb': ['174f87a3-f61d-4bba-9cea-02d82c1dd5e7'], '2871537b-5c0c-4ef3-8221-3662683e8c65': ['b642aa8f-53c0-456f-a0fd-dc8acec6e1c3'], '18af86eb-477b-4f98-a51d-99982a54a1d0': ['b642aa8f-53c0-456f-a0fd-dc8acec6e1c3'], '25285c9b-1a9f-4161-a8f0-e2cff56c35a6': ['793bdff1-4c6b-4bfa-8281-7a4148ea4a18'], '491f0e60-3f35-4a4e-b912-cbdf74a503fe': ['793bdff1-4c6b-4bfa-8281-7a4148ea4a18'], '7ab1f9a7-4e7d-417e-8af8-02edb56b2180': ['dc69df84-e73b-461c-acf4-affc6b7cecaa'], '4249c70e-e819-4a7e-abc3-c60793f2e75a': ['dc69df84-e73b-461c-acf4-affc6b7cecaa'], '6ae6776e-01da-4e43-89bf-720d2766bea6': ['45c77b88-1666-4da2-81e3-b2a58f57ba2d'], 'c877a3b7-6e8d-441a-94eb-d2f5058aaf85': ['45c77b88-1666-4da2-81e3-b2a58f57ba2d'], 'aba199c3-5b6c-4f2c-810d-1548fbb1af34': ['a3deb4ef-a726-4311-84bf-230531ef2ed3'], 'dd2ee359-2938-47a5-820d-1cff10b7c42f': ['a3deb4ef-a726-4311-84bf-230531ef2ed3'], '462b3993-5453-4d4a-a71d-4bdcecf675b8': ['27c12a97-a743-48ff-88ec-60d475280df5'], '94ce2715-b85a-4b9d-a606-6b7d40badb84': ['27c12a97-a743-48ff-88ec-60d475280df5'], '3da70132-ecb1-4270-8442-114da659a535': ['a2e35364-f812-4500-9bc6-7b4cedca9a63'], '13b40c4b-a1f7-41d4-b4bf-955cda1e4669': ['a2e35364-f812-4500-9bc6-7b4cedca9a63'], '7c903a60-6acd-4606-ae98-ac35ac2ce623': ['ba1f25a0-1d44-49a4-9d3a-1fc24b6827b6'], '5bd00fb4-95de-4a18-a519-66fd99b5b799': ['ba1f25a0-1d44-49a4-9d3a-1fc24b6827b6'], '956c8a54-b521-480f-b92b-04a0943de1cc': ['c280ecd7-5076-4664-bd1f-429d5e4754bf'], 'e2e06cba-a459-4e68-a432-37e1d596ce00': ['c280ecd7-5076-4664-bd1f-429d5e4754bf'], '66f683bc-40fd-4562-ae50-062f698ba10d': ['905bdaf6-7d1d-4010-9ee1-da4f0f3d63c0'], 'c201348b-be5f-41b6-bc16-846aadcb53e7': ['905bdaf6-7d1d-4010-9ee1-da4f0f3d63c0'], 'e7ab7e01-6433-4dfa-8608-2e5813f4f9a6': ['fa237e8f-df63-4c5a-b833-476a6c3cc6df'], '9c937e71-75ca-4d23-a239-3ad0abc3522a': ['fa237e8f-df63-4c5a-b833-476a6c3cc6df'], 'e1eac613-f5f7-4dc0-b3e2-f84c14738ef7': ['d11447c7-c4d9-4283-8c5b-685648c5eeaf'], 'a77d4d33-f5ad-4109-bfe6-1078e5ba3c6c': ['d11447c7-c4d9-4283-8c5b-685648c5eeaf'], 'ae2453c3-ec1d-4e55-87b4-75dd3572ef6b': ['27f6aff5-8136-4e41-9a4d-cff64582e7f9'], 'ae3d3a47-3bae-4013-a11f-30edf67c8614': ['27f6aff5-8136-4e41-9a4d-cff64582e7f9']}\n"
          ]
        }
      ],
      "source": [
        "print(training_questions)\n",
        "print(training_relevant_contexts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FSTG0bb7w73"
      },
      "source": [
        "We'll use the function to generate training, validation, and test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIZm4CqGVzBx",
        "outputId": "65a7703a-c528-40f6-aff4-1be84902cfc4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:13<00:00,  1.16s/it]\n"
          ]
        }
      ],
      "source": [
        "val_questions, val_relevant_contexts = await create_questions(val_split_documents, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'b9f87c3c-4f25-4ceb-bab6-02225f621479': 'What significant event occurred in 2024 related to the term \"slop\"?', '1adc4ed1-5903-4f8c-a956-6acabd3592bb': 'In what month did the author expand on the tweet by @deepfates regarding the term \"slop\"?', '9e74bb98-bf26-4ad0-97a9-aea54474b8b6': 'What does the term \"slop\" refer to in the context of AI-generated content?', 'dcbb6f78-c8f9-4eb9-bf28-3742a213395f': 'How does the author compare the terms \"spam\" and \"slop\" in relation to unwanted content?', '3021a0d9-44a4-463b-bb41-9e974f4bd61b': 'What does the term \"slop\" refer to in the context of generative AI usage?', 'c813f1ed-6116-46be-91f4-a60f8f11e510': 'What is \"model collapse\" and when was it first described?', '66ce2915-0d3f-4647-9c5a-735840b0eacb': 'What is the concern regarding AI models degenerating due to AI-generated content?', 'd4927791-ea64-45ef-9438-0cf0e6ddfe5c': 'How are AI labs addressing the potential degeneration of models in response to synthetic content?', '08222fd2-57df-498c-b3bf-2ce2f0e7389e': 'What role does synthetic data play in the pretraining of models, particularly in the Phi series?', 'bed4b5a1-ca03-4a62-802d-ed45577991e4': 'How does synthetic data compare to organic data in terms of advantages?', 'fdd53d6f-2fbd-44fd-9f0c-0c6fad6457ba': \"How does the complexity of relationships between tokens in organic datasets affect a model's learning process?\", 'cee05f0f-1ad3-4011-b7ef-cc74fc5cda15': 'In what way does the prediction of each token by preceding tokens facilitate reasoning for a language model?', 'e8542a30-7edd-4e51-908c-d88fd9735bef': 'What technique is being used by labs to create training data for smaller models?', 'f79274f3-58ea-411c-98bf-4322555b6d59': 'How many synthetically generated examples were used in Meta’s Llama 3.3 70B fine-tuning?', '25b31c8d-1958-4467-b493-1ea411b1bf79': 'What analogy is used to describe LLMs in the context provided?', 'c08bcef7-1b62-4968-805b-ea0b4bda7c92': 'What factors influence the effectiveness of LLMs according to the context?', '3332b1da-7464-456f-ace4-134531dc33a3': 'What are some of the tools that different systems can apply to problems, as mentioned in the context?', '8fec6f81-5815-47d9-bec7-5bb350320e7e': 'Why is it important to understand CSP and CORS HTTP headers when building a Claude Artifact that interacts with an external API?', '420cb7c1-6a2f-4ee8-8890-41c6d849315b': \"What limitations do the models, including OpenAI's o1, still face despite their increased capabilities?\", 'cdc0d829-4e94-4bcb-b92a-4a5df8d0111a': 'How does the user experience with the default LLM chat UI compare to using a more familiar interface?', 'fdc8e5e9-9b47-4995-95d4-a1b58f49744d': 'What issues arise from end users developing inaccurate mental models of AI systems like ChatGPT?', 'f7c7c15f-fd2a-4ce3-b2ef-27b2e50cba2c': 'Why is it considered ludicrous to use a screenshot from ChatGPT as evidence in an argument?', '59d9f3af-e039-460b-a8f4-8ede5156f001': 'What are some reasons why better informed people have chosen to avoid using LLMs?', 'df863302-4151-441e-8eb3-6b2c704d4bfc': 'Why is it important to develop skills for working with inherently unreliable technology like LLMs?'}\n",
            "{'b9f87c3c-4f25-4ceb-bab6-02225f621479': ['46fc8e69-be9e-42a0-9b18-d0351bc9425f'], '1adc4ed1-5903-4f8c-a956-6acabd3592bb': ['46fc8e69-be9e-42a0-9b18-d0351bc9425f'], '9e74bb98-bf26-4ad0-97a9-aea54474b8b6': ['d8e3b02c-7b95-408f-83d2-5ac657eebabc'], 'dcbb6f78-c8f9-4eb9-bf28-3742a213395f': ['d8e3b02c-7b95-408f-83d2-5ac657eebabc'], '3021a0d9-44a4-463b-bb41-9e974f4bd61b': ['c2de0069-ed64-4a71-99d1-f6b47d225861'], 'c813f1ed-6116-46be-91f4-a60f8f11e510': ['c2de0069-ed64-4a71-99d1-f6b47d225861'], '66ce2915-0d3f-4647-9c5a-735840b0eacb': ['e130e018-0e6e-4206-9c44-bc170c8d8e9f'], 'd4927791-ea64-45ef-9438-0cf0e6ddfe5c': ['e130e018-0e6e-4206-9c44-bc170c8d8e9f'], '08222fd2-57df-498c-b3bf-2ce2f0e7389e': ['8dec8e40-670b-4be2-b606-4434c969c0e4'], 'bed4b5a1-ca03-4a62-802d-ed45577991e4': ['8dec8e40-670b-4be2-b606-4434c969c0e4'], 'fdd53d6f-2fbd-44fd-9f0c-0c6fad6457ba': ['604df24c-1866-4bba-8704-1fed792c827d'], 'cee05f0f-1ad3-4011-b7ef-cc74fc5cda15': ['604df24c-1866-4bba-8704-1fed792c827d'], 'e8542a30-7edd-4e51-908c-d88fd9735bef': ['30ab419f-a53a-4258-9034-0d410a2b51f3'], 'f79274f3-58ea-411c-98bf-4322555b6d59': ['30ab419f-a53a-4258-9034-0d410a2b51f3'], '25b31c8d-1958-4467-b493-1ea411b1bf79': ['c75d57a3-05ed-40a9-9d0e-45efee6f276b'], 'c08bcef7-1b62-4968-805b-ea0b4bda7c92': ['c75d57a3-05ed-40a9-9d0e-45efee6f276b'], '3332b1da-7464-456f-ace4-134531dc33a3': ['7df54648-3126-4ab5-a4a2-3cdc371904ae'], '8fec6f81-5815-47d9-bec7-5bb350320e7e': ['7df54648-3126-4ab5-a4a2-3cdc371904ae'], '420cb7c1-6a2f-4ee8-8890-41c6d849315b': ['9a0c7e55-4b92-4af3-a3ca-12327d2cd040'], 'cdc0d829-4e94-4bcb-b92a-4a5df8d0111a': ['9a0c7e55-4b92-4af3-a3ca-12327d2cd040'], 'fdc8e5e9-9b47-4995-95d4-a1b58f49744d': ['26bd58ca-b533-4b95-ac66-ee6b69036624'], 'f7c7c15f-fd2a-4ce3-b2ef-27b2e50cba2c': ['26bd58ca-b533-4b95-ac66-ee6b69036624'], '59d9f3af-e039-460b-a8f4-8ede5156f001': ['2feeaedc-b936-48e4-8ac9-6307b7ae18ac'], 'df863302-4151-441e-8eb3-6b2c704d4bfc': ['2feeaedc-b936-48e4-8ac9-6307b7ae18ac']}\n"
          ]
        }
      ],
      "source": [
        "print(val_questions)\n",
        "print(val_relevant_contexts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6qUHg9sV2_y",
        "outputId": "b03bf5c6-d392-40bf-a061-1daceba2962e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:13<00:00,  1.09s/it]\n"
          ]
        }
      ],
      "source": [
        "test_questions, test_relevant_contexts = await create_questions(test_split_documents, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_jYOnAI43zK"
      },
      "source": [
        "### Reformating and Saving Datasets\n",
        "\n",
        "Now, we can save our datasets for later use!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "iF6IFFq9VsNu"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "training_corpus = {train_item.metadata[\"id\"] : train_item.page_content for train_item in training_split_documents}\n",
        "\n",
        "train_dataset = {\n",
        "    \"questions\" : training_questions,\n",
        "    \"relevant_contexts\" : training_relevant_contexts,\n",
        "    \"corpus\" : training_corpus\n",
        "}\n",
        "\n",
        "# pretty print the training dataset\n",
        "# json.dumps(train_dataset['corpus'], indent=2)\n",
        "\n",
        "\n",
        "with open(\"training_dataset.jsonl\", \"w\") as f:\n",
        "  json.dump(train_dataset, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "PqF9WaueV-V8"
      },
      "outputs": [],
      "source": [
        "val_corpus = {val_item.metadata[\"id\"] : val_item.page_content for val_item in val_split_documents}\n",
        "\n",
        "val_dataset = {\n",
        "    \"questions\" : val_questions,\n",
        "    \"relevant_contexts\" : val_relevant_contexts,\n",
        "    \"corpus\" : val_corpus\n",
        "}\n",
        "\n",
        "with open(\"val_dataset.jsonl\", \"w\") as f:\n",
        "  json.dump(val_dataset, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "0DSQ7WMnWAu6"
      },
      "outputs": [],
      "source": [
        "train_corpus = {test_item.metadata[\"id\"] : test_item.page_content for test_item in test_split_documents}\n",
        "\n",
        "test_dataset = {\n",
        "    \"questions\" : test_questions,\n",
        "    \"relevant_contexts\" : test_relevant_contexts,\n",
        "    \"corpus\" : train_corpus\n",
        "}\n",
        "\n",
        "with open(\"test_dataset.jsonl\", \"w\") as f:\n",
        "  json.dump(test_dataset, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAwklqQCgVi-"
      },
      "source": [
        "## Task 4: Fine-tuning `snowflake-arctic-embed-l`\n",
        "\n",
        "Now that we have a dataset, let's grab a `sentence-transformers` Embeddings model!\n",
        "\n",
        "We'll be using Snowflake's [`snowflake-arctic-embed-l`](https://huggingface.co/Snowflake/snowflake-arctic-embed-l) as a base embeddings model.\n",
        "\n",
        "It is a well performing embeddings model by itself, but there's a lot of very specific domain terms and vocabulary in our courpus - so lets fine-tune it and see what that can do for us!\n",
        "\n",
        ">> NOTE: Skip installing dependencies if you are running this notebook locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXzVHP3v1Cno",
        "outputId": "a9d6ca65-d355-460d-de89-7446a441512b"
      },
      "outputs": [],
      "source": [
        "#!pip install -qU sentence_transformers datasets pyarrow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-PGsQB7Xo6V",
        "outputId": "8df58392-a82b-45f9-ce4e-b4155522e2c6"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model_id = \"Snowflake/snowflake-arctic-embed-l\"\n",
        "model = SentenceTransformer(model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ztG07iB8CFO"
      },
      "source": [
        "We'll grab some necessary imports from `sentence_transformers` and `torch`.\n",
        "\n",
        "> NOTE: PyTorch (`torch`) is a popular machine learning library - while we don't go very deep into PyTorch it's an incredibly powerful and interesting library! Please read more about it [here](https://pytorch.org/tutorials/beginner/basics/intro.html)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "B-WbpuUWYFJr"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from sentence_transformers import InputExample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJtPPlck8HBE"
      },
      "source": [
        "We're using a toy batch size here to reflect the limited number of examples we have.\n",
        "\n",
        "> NOTE: It is typical to use a much larger batch size (~64+), hardware permitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "8Lokhy6KYHAv"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-6DT8hc8PmT"
      },
      "source": [
        "Let's move our dataset into the expected format for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "JJk37zQsYJ4P"
      },
      "outputs": [],
      "source": [
        "corpus = train_dataset['corpus']\n",
        "queries = train_dataset['questions']\n",
        "relevant_docs = train_dataset['relevant_contexts']\n",
        "\n",
        "examples = []\n",
        "for query_id, query in queries.items():\n",
        "    doc_id = relevant_docs[query_id][0]\n",
        "    text = corpus[doc_id]\n",
        "    example = InputExample(texts=[query, text])\n",
        "    examples.append(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjFx7KHI8TL0"
      },
      "source": [
        "Now we can create a `torch` `DataLoader`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "tiizmeIqZ_-w"
      },
      "outputs": [],
      "source": [
        "loader = DataLoader(\n",
        "    examples, batch_size=BATCH_SIZE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vA8rzlX8XbT"
      },
      "source": [
        "Next up, we'll prepare our loss function!\n",
        "\n",
        "Loss is an important part of training, fine-tuning, and more. If you want a deep dive on loss - you can check out our [event on loss!](https://www.youtube.com/watch?v=iB8FWR9aD5Q&t=8s).\n",
        "\n",
        "The core loss we're using today is called `MultipleNegativesRankingLoss` - you can find more information [here](https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/losses/MultipleNegativesRankingLoss.py).\n",
        "\n",
        "This is \"wrapped\" in `MatryoshkaLoss`, which you can read the implementation of [here](https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/losses/MatryoshkaLoss.py)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Uga4nnBqlVeh"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers.losses import MatryoshkaLoss, MultipleNegativesRankingLoss\n",
        "\n",
        "matryoshka_dimensions = [768, 512, 256, 128, 64]\n",
        "inner_train_loss = MultipleNegativesRankingLoss(model)\n",
        "train_loss = MatryoshkaLoss(\n",
        "    model, inner_train_loss, matryoshka_dims=matryoshka_dimensions\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJG4fOm66PHI"
      },
      "source": [
        "##### 🏗️ Activity #2:\n",
        "\n",
        "Both of these losses sound \"cool\", but what are they - exactly - under the hood?\n",
        "\n",
        "> A: MatryoshkaLoss is a specialized loss function used in embedding model training, particularly for Retrieval-Augmented Generation (RAG) systems. Named after Russian Matryoshka dolls (nested dolls), this loss function works with multiple levels of embedding spaces simultaneously.\n",
        "> \n",
        "> The key concept behind MatryoshkaLoss is that it creates nested embedding spaces of different dimensionalities, similar to how Matryoshka dolls nest inside each other. Each smaller embedding space is contained within larger ones, allowing the model to learn representations at multiple granularities.\n",
        "> \n",
        "> This approach offers several advantages:\n",
        "> \n",
        "> - Flexibility: Different downstream tasks can use different embedding dimensions from the same model\n",
        "> - Efficiency: The model can produce embeddings of varying sizes without requiring separate training\n",
        "> - Performance: Lower dimensional embeddings retain important information from higher dimensions\n",
        ">\n",
        "> In practice, MatryoshkaLoss helps create more robust and versatile embedding models that can adapt to different use cases while maintaining computational efficiency. It's particularly valuable in RAG systems where embedding quality directly impacts retrieval performance.\n",
        "\n",
        "> A: MultipleNegativesRankingLoss is a crucial loss function commonly used in training neural networks for information retrieval and sentence embedding tasks. It's particularly effective for RAG (Retrieval-Augmented Generation) systems.\n",
        ">\n",
        "> Here's how it works:\n",
        ">\n",
        "> - Basic Concept: The loss function takes a batch of (query, positive document) pairs and treats all other documents in the batch as negative examples for each query.\n",
        "> \n",
        "> - Training Process:\n",
        ">   - For each query, one document is marked as the correct (positive) match\n",
        ">   - All other documents in the batch are treated as incorrect (negative) matches\n",
        ">   - The model learns to score the positive pair higher than the negative pairs\n",
        "> \n",
        "> - Mathematical Implementation:\n",
        ">   - Calculates similarity scores between each query and all documents\n",
        ">   - Applies softmax across these scores\n",
        ">   - Maximizes the score for positive pairs while minimizing scores for negative pairs\n",
        "> \n",
        "> This approach is computationally efficient because it:\n",
        "> \n",
        "> - Reuses samples within the batch as negative examples\n",
        "> - Doesn't require explicit negative sampling\n",
        "> - Scales well with batch size (larger batches = more negative examples)> \n",
        "\n",
        "Why are these losses specifically doing? Please write a short summary of each loss.\n",
        "\n",
        "> A: See above\n",
        "\n",
        "> NOTE: This is a course focused on AI Engineering and the application of AI - looking for a hint? Try pasting the code (linked above) into ChatGPT/Claude to write the summary!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKxRuXfH844c"
      },
      "source": [
        "Now we can set-up our evaluator.\n",
        "\n",
        "> NOTE: Due to the formatting of our dataset - this is all we have to do!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "f0hAFwUyaHQG"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
        "\n",
        "corpus = val_dataset['corpus']\n",
        "queries = val_dataset['questions']\n",
        "relevant_docs = val_dataset['relevant_contexts']\n",
        "\n",
        "evaluator = InformationRetrievalEvaluator(queries, corpus, relevant_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYfap_ct8-bU"
      },
      "source": [
        "We'll train this model for 5 epochs, though you could increase this number if we had a significant amount more data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "svZG0pBHiQr6"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxitWoNX9DwW"
      },
      "source": [
        "It's training time!\n",
        "\n",
        "> NOTE: We're manually defining a warm-up period here - this is just to provide a smooth ramp into our training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dummy/dummy/runs/fwepdf10?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x17ade81a0>"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.init(mode=\"disabled\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 753,
          "referenced_widgets": [
            "0d3fc6edfdab4fe9aff8e805632eadad",
            "ebe8aa7a82124b57bce2d826c1dea0fa",
            "aa11c10b4234452d9430744ab89b59a2",
            "ca05cbcd72cb41d9856804ffb17b26cb",
            "cc2a33e9a7ac4c5699346fcbf53b7c95",
            "408f4dfce21a45cfad36047677ec8658",
            "ca5804644ef345c1b4f670fb7f088fe8",
            "2a872283afaa4a33a9bc3f9e57b3650b",
            "fe4d1052824c4ed29c38c5311087e650",
            "e283b1608c4d4266a03c57dc95aabb2e",
            "bfc4997e3bd94e66bebaa1ffdae1b99e"
          ]
        },
        "id": "aDhUHZY-iR09",
        "outputId": "6dbd9320-f7b9-46d6-b891-efdd12086631"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "915ca2d8950c44d18d5f145d243f15b2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='36' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 36/160 17:45 < 1:04:45, 0.03 it/s, Epoch 2.19/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Cosine Accuracy@1</th>\n",
              "      <th>Cosine Accuracy@3</th>\n",
              "      <th>Cosine Accuracy@5</th>\n",
              "      <th>Cosine Accuracy@10</th>\n",
              "      <th>Cosine Precision@1</th>\n",
              "      <th>Cosine Precision@3</th>\n",
              "      <th>Cosine Precision@5</th>\n",
              "      <th>Cosine Precision@10</th>\n",
              "      <th>Cosine Recall@1</th>\n",
              "      <th>Cosine Recall@3</th>\n",
              "      <th>Cosine Recall@5</th>\n",
              "      <th>Cosine Recall@10</th>\n",
              "      <th>Cosine Ndcg@10</th>\n",
              "      <th>Cosine Mrr@10</th>\n",
              "      <th>Cosine Map@100</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.963789</td>\n",
              "      <td>0.951389</td>\n",
              "      <td>0.951389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>No log</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.963789</td>\n",
              "      <td>0.951389</td>\n",
              "      <td>0.951389</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "RuntimeError",
          "evalue": "MPS backend out of memory (MPS allocated: 14.40 GB, other allocations: 3.70 GB, max allowed: 18.13 GB). Tried to allocate 119.23 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[45], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m warmup_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(loader) \u001b[38;5;241m*\u001b[39m EPOCHS \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_objectives\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfinetuned_arctic_ft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/src/github.com/thomfoolery/llmops/AIE5/09_Finetuning_Embeddings/.venv/lib/python3.13/site-packages/sentence_transformers/fit_mixin.py:385\u001b[0m, in \u001b[0;36mFitMixin.fit\u001b[0;34m(self, train_objectives, evaluator, epochs, steps_per_epoch, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, show_progress_bar, checkpoint_path, checkpoint_save_steps, checkpoint_save_total_limit)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    383\u001b[0m     trainer\u001b[38;5;241m.\u001b[39madd_callback(SaveModelCallback(output_path, evaluator, save_best_model))\n\u001b[0;32m--> 385\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/src/github.com/thomfoolery/llmops/AIE5/09_Finetuning_Embeddings/.venv/lib/python3.13/site-packages/transformers/trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/src/github.com/thomfoolery/llmops/AIE5/09_Finetuning_Embeddings/.venv/lib/python3.13/site-packages/transformers/trainer.py:2531\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2524\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2525\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2527\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2528\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2529\u001b[0m )\n\u001b[1;32m   2530\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2531\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2534\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2535\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2536\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2537\u001b[0m ):\n\u001b[1;32m   2538\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2539\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
            "File \u001b[0;32m~/src/github.com/thomfoolery/llmops/AIE5/09_Finetuning_Embeddings/.venv/lib/python3.13/site-packages/transformers/trainer.py:3712\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_accepts_loss_kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3710\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n\u001b[0;32m-> 3712\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3714\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
            "File \u001b[0;32m~/src/github.com/thomfoolery/llmops/AIE5/09_Finetuning_Embeddings/.venv/lib/python3.13/site-packages/accelerate/accelerator.py:2246\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2244\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2245\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2246\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/src/github.com/thomfoolery/llmops/AIE5/09_Finetuning_Embeddings/.venv/lib/python3.13/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/src/github.com/thomfoolery/llmops/AIE5/09_Finetuning_Embeddings/.venv/lib/python3.13/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/src/github.com/thomfoolery/llmops/AIE5/09_Finetuning_Embeddings/.venv/lib/python3.13/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 14.40 GB, other allocations: 3.70 GB, max allowed: 18.13 GB). Tried to allocate 119.23 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
          ]
        }
      ],
      "source": [
        "warmup_steps = int(len(loader) * EPOCHS * 0.1)\n",
        "\n",
        "model.fit(\n",
        "    train_objectives=[(loader, train_loss)],\n",
        "    epochs=EPOCHS,\n",
        "    warmup_steps=warmup_steps,\n",
        "    output_path='finetuned_arctic_ft',\n",
        "    show_progress_bar=True,\n",
        "    evaluator=evaluator,\n",
        "    evaluation_steps=50\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "58699484312f460cb96ac44e3af14aa2",
            "d45a7d95e5b14a6f88d5798fec9c40a7",
            "b011a6ccf8e745c6be6f3a17b7d61dce",
            "1016780729b04ba489d488922173ddae",
            "9d34dac405fd40139d5dc04eecef55ed",
            "d6169577ffb341a69eb0175e301b6a44",
            "d4acc9a7aca04564bfaefe33de079394",
            "4e2c257232c3472697e03350de43cb30",
            "381c780ce17e4662981175400fb8a0f8",
            "878dc96f87dd45f389948a546db33e94",
            "006bf6f5ebaf400486a6b82610381db0",
            "9198dd0fa8f04aa7b75307aa2d513bfb",
            "59cd26ae53024fbd85431b6683cd119c",
            "4e7ccd97042c4fb9a201b6dc76762e04",
            "72ec09e2cbbf4788b1561abfcdd0819a",
            "fb1e19624fde4d3c8048ce00264d6056",
            "9638a0456e0c41b1b9b9757932f55c53",
            "18825dc83221412ab602830fc00db71b",
            "a6ea48a80c194d128959422368aa0e10",
            "9f4026c62c60493caa18c014ae414e65"
          ]
        },
        "id": "b3iwclvyRD8L",
        "outputId": "1471e984-9351-478c-de34-6eb32263fa30"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "45935e9e3c33447f841fcd129b80768d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pn-Y6yjRoHk"
      },
      "outputs": [],
      "source": [
        "hf_username = \"llm-wizard\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "e5c86e0e33264ed8b6325e44f9421653",
            "acf3991e5d644f468264f561b28b514a",
            "4a723c6a56e94309b2e3abe63f28aedc",
            "ef25768d3b0746b48c6d02e9bd151bf9",
            "cc1f0bc4a1a74568b9c74a7392a1568f",
            "95160a05de5b402b9bdb0bd2d099cd00",
            "cf376b0ea3544055b8867d7013526502",
            "869814ecd49e46f9a33dd53130e6953f",
            "cc7d460d3c5a4ac6a9b64929736d6598",
            "13e9bf583f6442d395334947379a280a",
            "1995b4d98fe044e38f1980d47033ca40"
          ]
        },
        "id": "Nqhf3zWa9AiJ",
        "outputId": "c601b2a8-f8e9-4d71-9c7b-8ea4999ff077"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "model.safetensors: 100%|██████████| 1.34G/1.34G [00:35<00:00, 38.0MB/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'https://huggingface.co/llm-wizard/legal-ft-2/commit/fa3bc1d917e6c0beef1e06b3d31c4abd0899b25b'"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.push_to_hub(f\"{hf_username}/legal-ft-2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bo0zW5k9Poq"
      },
      "source": [
        "## Task 5: Evaluating our Retriever\n",
        "\n",
        "Now that we have fine-tuned our retriever - let's see if it's worthwhile!\n",
        "\n",
        "We'll start with some basic imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vq-2oqU0wHFr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "from langchain_core.documents import Document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jD0qrIh9X8f"
      },
      "source": [
        "Now we'll define a function that will help us evaluate our retrieval process.\n",
        "\n",
        "> NOTE: We're assuming 1 correct document in a \"hit\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0713_3cowX4q"
      },
      "outputs": [],
      "source": [
        "def evaluate_openai(\n",
        "    dataset,\n",
        "    embed_model,\n",
        "    top_k=5,\n",
        "    verbose=False,\n",
        "):\n",
        "  corpus = dataset['corpus']\n",
        "  questions = dataset['questions']\n",
        "  relevant_docs = dataset['relevant_contexts']\n",
        "  documents = [Document(page_content=content, metadata={\"id\": doc_id}) for doc_id, content in corpus.items()]\n",
        "  vectorstore = FAISS.from_documents(documents, embed_model)\n",
        "\n",
        "  retriever = vectorstore.as_retriever(search_kwargs={\"k\": top_k})\n",
        "\n",
        "  eval_results = []\n",
        "  for id, question in tqdm(questions.items()):\n",
        "    retrieved_nodes = retriever.invoke(question)\n",
        "    retrieved_ids = [node.metadata[\"id\"] for node in retrieved_nodes]\n",
        "    expected_id = relevant_docs[id][0]\n",
        "    is_hit = expected_id in retrieved_ids\n",
        "    eval_results.append({\"id\": id, \"question\": question, \"expected_id\": expected_id, \"is_hit\": is_hit})\n",
        "\n",
        "  return eval_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOr49m4O9lxY"
      },
      "source": [
        "All that's left to do is evaluate, we'll evaluate our model against:\n",
        "\n",
        "1. OpenAI's closed source `text-embedding-3-small`\n",
        "2. The base non-fine-tuned version of `Snowflake/snowflake-arctic-embed-l`.\n",
        "\n",
        "Let's see how it stacks up!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijaeYpf593IW"
      },
      "source": [
        "### `text-embedding-3-small`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyY3PztaxnU3",
        "outputId": "5a1ec5e9-00fc-4140-d5b6-aa752dd4c9fa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 24/24 [00:07<00:00,  3.19it/s]\n"
          ]
        }
      ],
      "source": [
        "te3_openai = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "te3_results = evaluate_openai(test_dataset, te3_openai)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkyW90TCxx_i"
      },
      "outputs": [],
      "source": [
        "te3_results_df = pd.DataFrame(te3_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MscVRdNCylJ-",
        "outputId": "275beff8-3c59-4063-8270-c01736b4ee05"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(1.0)"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "te3_hit_rate = te3_results_df[\"is_hit\"].mean()\n",
        "te3_hit_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ra-mh0L96dQ"
      },
      "source": [
        "### `Snowflake/snowflake-arctic-embed-l` (base)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEskxwvFypHe",
        "outputId": "a3aad8ce-48ef-4d8f-9ed0-122b1ec9a000"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 24/24 [00:00<00:00, 123.63it/s]\n"
          ]
        }
      ],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "huggingface_embeddings = HuggingFaceEmbeddings(model_name=\"Snowflake/snowflake-arctic-embed-l\")\n",
        "arctic_embed_m_results = evaluate_openai(test_dataset, huggingface_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlKgiXTWzMTg"
      },
      "outputs": [],
      "source": [
        "arctic_embed_m_results_df = pd.DataFrame(arctic_embed_m_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zV5vJWrJzOhc",
        "outputId": "30049be6-deb9-4ceb-dc13-24d7e125f131"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(0.9166666666666666)"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "arctic_embed_m_hit_rate = arctic_embed_m_results_df[\"is_hit\"].mean()\n",
        "arctic_embed_m_hit_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcR3-0s19_lu"
      },
      "source": [
        "### `Snowflake/snowflake-arctic-embed-l` (fine-tuned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ilse1LduzP1i",
        "outputId": "292ab58f-7594-45fc-a5b8-1062cc553f66"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertModel were not initialized from the model checkpoint at finetuned_arctic_ft and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100%|██████████| 24/24 [00:00<00:00, 111.82it/s]\n"
          ]
        }
      ],
      "source": [
        "finetune_embeddings = HuggingFaceEmbeddings(model_name=\"finetuned_arctic_ft\")\n",
        "finetune_results = evaluate_openai(test_dataset, finetune_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxhZPqkNzZlh"
      },
      "outputs": [],
      "source": [
        "finetune_results_df = pd.DataFrame(finetune_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4thAK2BXzaj6",
        "outputId": "e890b5d1-86b7-4bfe-8ffe-a779a132e0c6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(1.0)"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "finetune_hit_rate = finetune_results_df[\"is_hit\"].mean()\n",
        "finetune_hit_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iegFM209mBk3"
      },
      "source": [
        "## Task 1: Vibe Checking the RAG Pipeline\n",
        "\n",
        "We're going to use our RAG pipeline to vibe check on some common phrases now that we've modified it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xzg0AA5krgR4"
      },
      "source": [
        "### Creating New Chunks\n",
        "\n",
        "In order to try and evaluate our system more fairly, let's create new chunks that we will use to create our Vector Store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwQ2_LqNr0Tw"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 600,\n",
        "    chunk_overlap  = 50,\n",
        "    length_function = len\n",
        ")\n",
        "\n",
        "training_documents = text_splitter.split_documents(text_loader.load())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIdxahHXpP-c"
      },
      "source": [
        "### Base Chain\n",
        "\n",
        "We'll start by constructing our base chain, which will use the untrained retrieval model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOsxIXpNpWC2"
      },
      "source": [
        "#### R - Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azIGIKYfmNCT"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "base_vectorstore = FAISS.from_documents(training_documents, huggingface_embeddings)\n",
        "base_retriever = base_vectorstore.as_retriever(search_kwargs={\"k\": 6})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-1nVZ0KpX5N"
      },
      "source": [
        "#### A - Augmented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G10Fr-aKojeA"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_PROMPT = \"\"\"\\\n",
        "Given a provided context and a question, you must answer the question. If you do not know the answer, you must state that you do not know.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt_template = ChatPromptTemplate.from_template(RAG_PROMPT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Euq6RQEopZvD"
      },
      "source": [
        "#### G - Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-mfbbrypMHG"
      },
      "outputs": [],
      "source": [
        "rag_llm =  ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ2p4mnUpbYY"
      },
      "source": [
        "#### RAG - LCEL RAG Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssuR-LaboyGq"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "\n",
        "base_rag_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | base_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt_template | rag_llm | StrOutputParser(), \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "emm6WbB9pfKt",
        "outputId": "f0e0c83f-c617-493e-99ce-869061a315f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'An agent, in the context of AI, is an infuriatingly vague term that generally refers to AI systems that can act on your behalf. There are two main interpretations: one sees agents as systems that go and perform tasks for you (like a travel agent), while the other views them as LLMs (large language models) that have access to tools and can run processes to solve problems. However, the term lacks a clear and widely understood definition, leading to confusion about its meaning and utility.'"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_rag_chain.invoke({\"question\" : \"What is an agent?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "mUOrd0OBprAq",
        "outputId": "0070d677-0bde-48be-a8a3-f0c53b9eee90"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Organizations that have produced better-than-GPT-3 class models include Anthropic, Mistral, Google, Meta, EleutherAI, Stability AI, TII in Abu Dhabi (Falcon), Microsoft Research, xAI, Replit, Baidu, and several others.'"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_rag_chain.invoke({\"question\" : \"Who has produced better models than GPT-3?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "OnfuFl59py7I",
        "outputId": "37480e29-17a6-40e2-fe8a-7eb6ea270569"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'I do not know.'"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_rag_chain.invoke({\"question\" : \"What is the laziest AI month?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "-NmqwHBDqTZ8",
        "outputId": "86d7b59a-97c6-4b65-805f-ae449e3b1a20"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'I do not know.'"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_rag_chain.invoke({\"question\" : \"What is the largest model that Simon has run on his phone?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqNS0UJAp3lC"
      },
      "source": [
        "### Fine-tuned Embedding Model\n",
        "\n",
        "Now let's rebuild our RAG chain with the Fine-tuned model - the only component we need to change is our `FAISS` vectorstore!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihO7tP6mqATy"
      },
      "outputs": [],
      "source": [
        "finetune_vectorstore = FAISS.from_documents(training_documents, finetune_embeddings)\n",
        "finetune_retriever = finetune_vectorstore.as_retriever(search_kwargs={\"k\": 6})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_cIFvWzqKGY"
      },
      "outputs": [],
      "source": [
        "finetune_rag_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | finetune_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt_template | rag_llm | StrOutputParser(), \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "OJmRHJF2qNgj",
        "outputId": "b99d4b58-8487-48ec-ee6c-8ea93f9b0192"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'An \"agent\" in the context of AI refers to a system that can act on your behalf, but the term is vague and lacks a single, clear definition. There are different interpretations, with some viewing agents as systems that autonomously perform tasks (like a travel agent), while others see them as LLMs (large language models) that utilize tools to solve problems. The concept is complicated by issues of gullibility, as these systems may struggle to distinguish truth from fiction, which affects their utility.'"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "finetune_rag_chain.invoke({\"question\" : \"What is an Agent?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "EnK-c2ugqPPh",
        "outputId": "b8300e0a-1b51-48dd-93c3-254e0aa84e36"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Organizations that have produced better-than-GPT-3 class models include Anthropic, Mistral, Google, Meta, EleutherAI, Stability AI, TII in Abu Dhabi (Falcon), Microsoft Research, xAI, Replit, Baidu, and several others.'"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "finetune_rag_chain.invoke({\"question\" : \"Who has produced better models than GPT-3?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "83hssg1AWozc",
        "outputId": "8d1ef134-7889-4379-a49b-6c0a476b7a1a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The laziest AI month is suggested to be December, as it is mentioned that ChatGPT may get lazy in December due to its hidden system prompt including the current date and the observation that people provide less useful answers coming up to the holidays.'"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "finetune_rag_chain.invoke({\"question\" : \"What is the laziest AI month?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "rsHmGeFbqRET",
        "outputId": "1ab223c2-d2fc-46ac-8541-af038de3166d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Simon has run the Llama 3.2 3B model on his iPhone.'"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "finetune_rag_chain.invoke({\"question\" : \"What is the largest model that Simon has run on his phone?\"})[\"response\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDgD8seY_I3W"
      },
      "source": [
        "####❓Question #2:\n",
        "\n",
        "Which LCEL RAG Chain do you think answered the questions better, and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCbq1sZArIx4"
      },
      "source": [
        "## Task 2: RAGAS Evaluation\n",
        "\n",
        "It's great to have some idea of how our system is doing based on vibe-checks, but let's use RAGAS to provide more insight info. on how things are improving!\n",
        "\n",
        "> NOTE: Please recreate *exactly* the RAGAS process we used to evaluate RAG, baselining with the default retriever, and then comparing the new retriever. The includes the Synthetic Data Generation steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jq880DtHk9pX"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "006bf6f5ebaf400486a6b82610381db0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d3fc6edfdab4fe9aff8e805632eadad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ebe8aa7a82124b57bce2d826c1dea0fa",
              "IPY_MODEL_aa11c10b4234452d9430744ab89b59a2",
              "IPY_MODEL_ca05cbcd72cb41d9856804ffb17b26cb"
            ],
            "layout": "IPY_MODEL_cc2a33e9a7ac4c5699346fcbf53b7c95"
          }
        },
        "1016780729b04ba489d488922173ddae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "CheckboxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_9198dd0fa8f04aa7b75307aa2d513bfb",
            "style": "IPY_MODEL_59cd26ae53024fbd85431b6683cd119c",
            "value": true
          }
        },
        "13e9bf583f6442d395334947379a280a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18825dc83221412ab602830fc00db71b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6ea48a80c194d128959422368aa0e10",
            "placeholder": "​",
            "style": "IPY_MODEL_9f4026c62c60493caa18c014ae414e65",
            "value": "Connecting..."
          }
        },
        "1995b4d98fe044e38f1980d47033ca40": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a872283afaa4a33a9bc3f9e57b3650b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "381c780ce17e4662981175400fb8a0f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "408f4dfce21a45cfad36047677ec8658": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a723c6a56e94309b2e3abe63f28aedc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_869814ecd49e46f9a33dd53130e6953f",
            "max": 1336413848,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cc7d460d3c5a4ac6a9b64929736d6598",
            "value": 1336413848
          }
        },
        "4e2c257232c3472697e03350de43cb30": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e7ccd97042c4fb9a201b6dc76762e04": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58699484312f460cb96ac44e3af14aa2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_d4acc9a7aca04564bfaefe33de079394"
          }
        },
        "59cd26ae53024fbd85431b6683cd119c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72ec09e2cbbf4788b1561abfcdd0819a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "869814ecd49e46f9a33dd53130e6953f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "878dc96f87dd45f389948a546db33e94": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9198dd0fa8f04aa7b75307aa2d513bfb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95160a05de5b402b9bdb0bd2d099cd00": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9638a0456e0c41b1b9b9757932f55c53": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d34dac405fd40139d5dc04eecef55ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_4e7ccd97042c4fb9a201b6dc76762e04",
            "style": "IPY_MODEL_72ec09e2cbbf4788b1561abfcdd0819a",
            "tooltip": ""
          }
        },
        "9f4026c62c60493caa18c014ae414e65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a6ea48a80c194d128959422368aa0e10": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa11c10b4234452d9430744ab89b59a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a872283afaa4a33a9bc3f9e57b3650b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fe4d1052824c4ed29c38c5311087e650",
            "value": 1
          }
        },
        "acf3991e5d644f468264f561b28b514a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95160a05de5b402b9bdb0bd2d099cd00",
            "placeholder": "​",
            "style": "IPY_MODEL_cf376b0ea3544055b8867d7013526502",
            "value": "model.safetensors: 100%"
          }
        },
        "b011a6ccf8e745c6be6f3a17b7d61dce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_878dc96f87dd45f389948a546db33e94",
            "placeholder": "​",
            "style": "IPY_MODEL_006bf6f5ebaf400486a6b82610381db0",
            "value": ""
          }
        },
        "bfc4997e3bd94e66bebaa1ffdae1b99e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca05cbcd72cb41d9856804ffb17b26cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e283b1608c4d4266a03c57dc95aabb2e",
            "placeholder": "​",
            "style": "IPY_MODEL_bfc4997e3bd94e66bebaa1ffdae1b99e",
            "value": " 0/1 [00:00&lt;?, ?example/s]"
          }
        },
        "ca5804644ef345c1b4f670fb7f088fe8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc1f0bc4a1a74568b9c74a7392a1568f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc2a33e9a7ac4c5699346fcbf53b7c95": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "cc7d460d3c5a4ac6a9b64929736d6598": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cf376b0ea3544055b8867d7013526502": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d45a7d95e5b14a6f88d5798fec9c40a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e2c257232c3472697e03350de43cb30",
            "placeholder": "​",
            "style": "IPY_MODEL_381c780ce17e4662981175400fb8a0f8",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "d4acc9a7aca04564bfaefe33de079394": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "d6169577ffb341a69eb0175e301b6a44": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb1e19624fde4d3c8048ce00264d6056",
            "placeholder": "​",
            "style": "IPY_MODEL_9638a0456e0c41b1b9b9757932f55c53",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "e283b1608c4d4266a03c57dc95aabb2e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5c86e0e33264ed8b6325e44f9421653": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_acf3991e5d644f468264f561b28b514a",
              "IPY_MODEL_4a723c6a56e94309b2e3abe63f28aedc",
              "IPY_MODEL_ef25768d3b0746b48c6d02e9bd151bf9"
            ],
            "layout": "IPY_MODEL_cc1f0bc4a1a74568b9c74a7392a1568f"
          }
        },
        "ebe8aa7a82124b57bce2d826c1dea0fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_408f4dfce21a45cfad36047677ec8658",
            "placeholder": "​",
            "style": "IPY_MODEL_ca5804644ef345c1b4f670fb7f088fe8",
            "value": "Computing widget examples:   0%"
          }
        },
        "ef25768d3b0746b48c6d02e9bd151bf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13e9bf583f6442d395334947379a280a",
            "placeholder": "​",
            "style": "IPY_MODEL_1995b4d98fe044e38f1980d47033ca40",
            "value": " 1.34G/1.34G [01:11&lt;00:00, 21.0MB/s]"
          }
        },
        "fb1e19624fde4d3c8048ce00264d6056": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe4d1052824c4ed29c38c5311087e650": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
